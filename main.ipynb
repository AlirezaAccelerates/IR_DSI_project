{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: jupyter [-h] [--version] [--config-dir] [--data-dir] [--runtime-dir]\n",
      "               [--paths] [--json] [--debug]\n",
      "               [subcommand]\n",
      "\n",
      "Jupyter: Interactive Computing\n",
      "\n",
      "positional arguments:\n",
      "  subcommand     the subcommand to launch\n",
      "\n",
      "options:\n",
      "  -h, --help     show this help message and exit\n",
      "  --version      show the versions of core jupyter packages and exit\n",
      "  --config-dir   show Jupyter config dir\n",
      "  --data-dir     show Jupyter data dir\n",
      "  --runtime-dir  show Jupyter runtime dir\n",
      "  --paths        show all Jupyter paths. Add --json for machine-readable\n",
      "                 format.\n",
      "  --json         output paths as machine-readable json\n",
      "  --debug        output debug information about paths\n",
      "\n",
      "Available subcommands: console dejavu events execute kernel kernelspec lab\n",
      "labextension labhub migrate nbconvert notebook qtconsole run server\n",
      "troubleshoot trust\n",
      "\n",
      "Jupyter command `jupyter-nbextension` not found.\n"
     ]
    }
   ],
   "source": [
    "#!pip install nltk\n",
    "#!pip install pytorch_lightning\n",
    "#!pip install gensim\n",
    "#!pip install pyserini==0.12.0\n",
    "#!pip install python-terrier\n",
    "#!pip install ipywidgets\n",
    "#!pip install --upgrade notebook jupyter\n",
    "#!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#from src.dataset import mydataset\n",
    "#from src.utils import utils, dataset_utils, eval_utils, trie\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/alirezarafiei/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/alirezarafiei/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/alirezarafiei/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 120\n",
    "MAX_TOKENS = 7\n",
    "K = 10      # K param metrics\n",
    "\n",
    "QUERIES_PATH = './materials/queries.json'\n",
    "DOCS_PATH = './materials/documents.json'\n",
    "W2V_PATH = './materials/word2vec_model.bin'\n",
    "BT_MOD_PATH = './materials/bt_model.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the json files of documents does not exists\n",
    "if not os.path.exists(QUERIES_PATH) or not os.path.exists(DOCS_PATH) or not os.path.exists(W2V_PATH):\n",
    "    # Build dictionaries and corpus\n",
    "    queries, documents, corpus = dataset_utils.build_dicts(\n",
    "        max_topics=None, max_docs=10)\n",
    "    # Write to file the dictionaries\n",
    "    with open(QUERIES_PATH, 'w') as json_file:\n",
    "        json.dump(queries, json_file)\n",
    "    with open(DOCS_PATH, 'w') as json_file:\n",
    "        json.dump(documents, json_file)\n",
    "\n",
    "    # Train the word2vec model\n",
    "    w2v_model = Word2Vec(\n",
    "        sentences=corpus, vector_size=EMBEDDING_SIZE,\n",
    "        window=MAX_TOKENS if MAX_TOKENS else 5,\n",
    "        min_count=1, sg=0, epochs=10)\n",
    "    \n",
    "    # Save w2v model\n",
    "    w2v_model.save(W2V_PATH)\n",
    "\n",
    "# Read from json\n",
    "with open(QUERIES_PATH, 'r') as json_file:\n",
    "    queries = json.load(json_file)\n",
    "with open(DOCS_PATH, 'r') as json_file:\n",
    "    documents = json.load(json_file)\n",
    "\n",
    "# Load model\n",
    "w2v_model = Word2Vec.load(W2V_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pyterrier-based approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTerrier 0.10.0 has loaded Terrier 5.8 (built by craigm on 2023-11-01 18:05) and terrier-helper 0.0.8\n",
      "\n",
      "No etc/terrier.properties, using terrier.default.properties for bootstrap configuration.\n"
     ]
    }
   ],
   "source": [
    "import pyterrier as pt\n",
    "if not pt.started():\n",
    "    pt.init()\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = pd.DataFrame([\n",
    "    {\"docno\": doc_id, \"text\": details['raw']}\n",
    "    for doc_id, details in documents.items()\n",
    "])\n",
    "\n",
    "indexer = pt.DFIndexer(\"./materials/index_pt/index\", overwrite=True)\n",
    "index_ref = indexer.index(docs[\"text\"], docs[\"docno\"])\n",
    "\n",
    "quer = pd.DataFrame([{\"qid\": str(qid), \"query\": details['raw']} for qid, details in queries.items()])\n",
    "qrels = pd.DataFrame([{\"qid\": str(qid), \"docno\": docno, \"label\": 1} for qid, details in queries.items() for docno in details['docids_list']])\n",
    "\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "def remove_punctuation(text):\n",
    "    text_no_punctuation = text.translate(translator)\n",
    "    return text_no_punctuation\n",
    "\n",
    "quer['query'] = quer['query'].apply(remove_punctuation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt.Experiment:   0%|          | 0/3 [00:00<?, ?system/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt.Experiment: 100%|██████████| 3/3 [03:05<00:00, 61.91s/system]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         name     map    ndcg     P@5    P@10    P@15    P@20    P@30  P@100  \\\n",
      "0    BR(BM25)  0.9456  0.9751  0.9614  0.9167  0.6399  0.4853  0.3264  0.099   \n",
      "1  BR(TF_IDF)  0.9458  0.9752  0.9617  0.9171  0.6400  0.4854  0.3264  0.099   \n",
      "2     BR(PL2)  0.9378  0.9720  0.9552  0.9043  0.6368  0.4843  0.3261  0.099   \n",
      "\n",
      "    P@200  ...  P@1000     R@5    R@10    R@15    R@20    R@30   R@100  \\\n",
      "0  0.0496  ...    0.01  0.4807  0.9167  0.9599  0.9706  0.9791  0.9896   \n",
      "1  0.0496  ...    0.01  0.4808  0.9171  0.9601  0.9708  0.9792  0.9896   \n",
      "2  0.0496  ...    0.01  0.4776  0.9043  0.9553  0.9686  0.9784  0.9896   \n",
      "\n",
      "    R@200   R@500  R@1000  \n",
      "0  0.9923  0.9944  0.9952  \n",
      "1  0.9922  0.9944  0.9952  \n",
      "2  0.9924  0.9944  0.9952  \n",
      "\n",
      "[3 rows x 21 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the index\n",
    "index = pt.IndexFactory.of(index_ref)\n",
    "\n",
    "# Create a BM25 retrieval model\n",
    "bm25 = pt.BatchRetrieve(index, wmodel=\"BM25\")\n",
    "\n",
    "tf_idf = pt.BatchRetrieve(index, wmodel=\"TF_IDF\")\n",
    "\n",
    "pl2 = pt.BatchRetrieve(index, wmodel=\"PL2\")\n",
    "\n",
    "evaluation = pt.Experiment(\n",
    "    [bm25, tf_idf, pl2],\n",
    "    quer,\n",
    "    qrels,\n",
    "    eval_metrics=['map', 'ndcg', 'P', 'recall'],\n",
    "    round=4,\n",
    "    verbose=\"true\"\n",
    ")\n",
    "\n",
    "print(evaluation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
