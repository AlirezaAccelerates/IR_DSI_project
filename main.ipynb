{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch-tb-profiler\n",
      "  Obtaining dependency information for torch-tb-profiler from https://files.pythonhosted.org/packages/46/d0/891ec43349f287ea5c313ebc1320e4eda38ccd4c7a0951657213467eaab5/torch_tb_profiler-0.4.3-py3-none-any.whl.metadata\n",
      "  Downloading torch_tb_profiler-0.4.3-py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: pandas>=1.0.0 in /Users/alirezarafiei/miniforge3/lib/python3.10/site-packages (from torch-tb-profiler) (2.2.0)\n",
      "Requirement already satisfied: tensorboard!=2.1.0,>=1.15 in /Users/alirezarafiei/miniforge3/lib/python3.10/site-packages (from torch-tb-profiler) (2.13.0)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /Users/alirezarafiei/miniforge3/lib/python3.10/site-packages (from pandas>=1.0.0->torch-tb-profiler) (1.24.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/alirezarafiei/miniforge3/lib/python3.10/site-packages (from pandas>=1.0.0->torch-tb-profiler) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/alirezarafiei/miniforge3/lib/python3.10/site-packages (from pandas>=1.0.0->torch-tb-profiler) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/alirezarafiei/miniforge3/lib/python3.10/site-packages (from pandas>=1.0.0->torch-tb-profiler) (2023.4)\n",
      "Requirement already satisfied: absl-py>=0.4 in /Users/alirezarafiei/miniforge3/lib/python3.10/site-packages (from tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /Users/alirezarafiei/miniforge3/lib/python3.10/site-packages (from tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (1.60.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/alirezarafiei/miniforge3/lib/python3.10/site-packages (from tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (2.26.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /Users/alirezarafiei/miniforge3/lib/python3.10/site-packages (from tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/alirezarafiei/miniforge3/lib/python3.10/site-packages (from tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (3.5.2)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in /Users/alirezarafiei/miniforge3/lib/python3.10/site-packages (from tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (4.23.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/alirezarafiei/miniforge3/lib/python3.10/site-packages (from tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (2.31.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/alirezarafiei/miniforge3/lib/python3.10/site-packages (from tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (69.0.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/alirezarafiei/miniforge3/lib/python3.10/site-packages (from tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/alirezarafiei/miniforge3/lib/python3.10/site-packages (from tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (3.0.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /Users/alirezarafiei/miniforge3/lib/python3.10/site-packages (from tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (0.42.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/alirezarafiei/miniforge3/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/alirezarafiei/miniforge3/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/alirezarafiei/miniforge3/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/alirezarafiei/miniforge3/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (1.3.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/alirezarafiei/miniforge3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->torch-tb-profiler) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/alirezarafiei/miniforge3/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/alirezarafiei/miniforge3/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/alirezarafiei/miniforge3/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/alirezarafiei/miniforge3/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (2023.11.17)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/alirezarafiei/miniforge3/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (2.1.4)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /Users/alirezarafiei/miniforge3/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/alirezarafiei/miniforge3/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (3.2.2)\n",
      "Downloading torch_tb_profiler-0.4.3-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torch-tb-profiler\n",
      "Successfully installed torch-tb-profiler-0.4.3\n"
     ]
    }
   ],
   "source": [
    "#!pip install nltk\n",
    "#!pip install pytorch_lightning\n",
    "#!pip install gensim\n",
    "#!pip install pyserini==0.12.0\n",
    "#!pip install python-terrier\n",
    "#!pip install ipywidgets\n",
    "#!pip install --upgrade notebook jupyter\n",
    "#!jupyter nbextension enable --py widgetsnbextension\n",
    "#!pip install tensorboard tensorboardX\n",
    "#!pip install torch-tb-profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from src.dataset import mydataset\n",
    "from src.utils import utils, dataset_utils, eval_utils, trie\n",
    "\n",
    "from src.models import siamese, attention_siamese, siamese_triplet, s2stransformer, s2s_transf, s2s_transf_semantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/alirezarafiei/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/alirezarafiei/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/alirezarafiei/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 120\n",
    "MAX_TOKENS = 7\n",
    "K = 10      # K param metrics\n",
    "\n",
    "QUERIES_PATH = './materials/queries.json'\n",
    "DOCS_PATH = './materials/documents.json'\n",
    "W2V_PATH = './materials/word2vec_model.bin'\n",
    "BT_MOD_PATH = './materials/bt_model.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the json files of documents does not exists\n",
    "if not os.path.exists(QUERIES_PATH) or not os.path.exists(DOCS_PATH) or not os.path.exists(W2V_PATH):\n",
    "    # Build dictionaries and corpus\n",
    "    queries, documents, corpus = dataset_utils.build_dicts(\n",
    "        max_topics=None, max_docs=10)\n",
    "    # Write to file the dictionaries\n",
    "    with open(QUERIES_PATH, 'w') as json_file:\n",
    "        json.dump(queries, json_file)\n",
    "    with open(DOCS_PATH, 'w') as json_file:\n",
    "        json.dump(documents, json_file)\n",
    "\n",
    "    # Train the word2vec model\n",
    "    w2v_model = Word2Vec(\n",
    "        sentences=corpus, vector_size=EMBEDDING_SIZE,\n",
    "        window=MAX_TOKENS if MAX_TOKENS else 5,\n",
    "        min_count=1, sg=0, epochs=10)\n",
    "    \n",
    "    # Save w2v model\n",
    "    w2v_model.save(W2V_PATH)\n",
    "\n",
    "# Read from json\n",
    "with open(QUERIES_PATH, 'r') as json_file:\n",
    "    queries = json.load(json_file)\n",
    "with open(DOCS_PATH, 'r') as json_file:\n",
    "    documents = json.load(json_file)\n",
    "\n",
    "# Load model\n",
    "w2v_model = Word2Vec.load(W2V_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding queries: 100%|██████████| 6980/6980 [00:00<00:00, 12721.11it/s]\n",
      "Embedding documents: 100%|██████████| 67869/67869 [00:33<00:00, 2022.98it/s]\n"
     ]
    }
   ],
   "source": [
    "# Compute w2v embeddings for queries\n",
    "for id in tqdm(queries, \"Embedding queries\"):\n",
    "    raw_query = queries[id]['raw']\n",
    "    queries[id]['emb'] = dataset_utils.compute_embedding(\n",
    "        raw_query, w2v_model, 1)\n",
    "    queries[id]['first_L_emb'] = dataset_utils.compute_embedding(\n",
    "        raw_query, w2v_model, MAX_TOKENS)\n",
    "\n",
    "# Compute w2v embeddings for documents\n",
    "for docid in tqdm(documents, \"Embedding documents\"):\n",
    "    raw_doc = documents[docid]['raw']\n",
    "    documents[docid]['emb'] = dataset_utils.compute_embedding(\n",
    "        raw_doc, w2v_model)\n",
    "    documents[docid]['first_L_emb'] = dataset_utils.compute_embedding(\n",
    "        raw_doc, w2v_model, MAX_TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08c8894fccf14672a171b81f67d1974e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building QueryDocumentDataset:   0%|          | 0/6980 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7af7081782fc4eb5b66ab8b44fabef9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building TripletQueryDocumentDataset:   0%|          | 0/6980 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create datasets for siamese models\n",
    "pairs_dataset = mydataset.QueryDocumentDataset(queries, documents)\n",
    "triplets_dataset = mydataset.TripletQueryDocumentDataset(queries, documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "883d8979cf194154857fce9eed2f2b74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building DocumentDataset:   0%|          | 0/67869 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a96bd2118a243ef9f4be4e088536704",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building RetrievalDataset:   0%|          | 0/6980 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create datasets for seq2seq models\n",
    "dd_dataset = mydataset.DocumentDataset(documents)\n",
    "ret_dataset = mydataset.RetrievalDataset(documents, queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(query, doc, relevance) = (1102330, 7867440, 1)\n",
      "(query, doc+, doc-) = (1102330, 7867440, 848780)\n",
      "\n",
      "doc_emb: tensor([ 5696,  8325,   819,  1836,  9657,  1406,  7178, 19511,   819,  1836,\n",
      "           78,    60,  9657, 20743,  3841,   706,   228,     3,  9052,   226,\n",
      "          159,    51,  3841, 20743,   158,    32,   102,    40, 20743,  3841,\n",
      "         3841,     1]) \n",
      "docid_emb: tensor([12,  7,  8,  6,  7,  4,  4,  6, 10, 11, 11])\n",
      "decoded docid: 7867446\n",
      "decoded doc: suffer frequent headach jaw pain wake dull headach sore jaw grind teeth night could bruxism teeth grind peopl grind teeth teeth</s>\n",
      "\n",
      "query_emb: tensor([  158,    32,   102,    40, 20743,  3841,  2085,     1,     0]) \n",
      "docid_emb: tensor([12,  7,  8,  6,  7,  4,  4,  6, 10, 11, 11])\n",
      "decoded docid: 7867446\n",
      "decoded query: peopl grind teeth sleep</s><pad>\n"
     ]
    }
   ],
   "source": [
    "# QueryDocumentDataset sample print (by default, IDs will be printed)\n",
    "print(\n",
    "    f\"(query, doc, relevance) = \"\n",
    "    f\"({pairs_dataset[0][0]}, {pairs_dataset[0][1]}, {pairs_dataset[0][2]})\"\n",
    ")\n",
    "\n",
    "# TripletQueryDocumentDataset sample print (by default, IDs will be printed)\n",
    "print(\n",
    "    f\"(query, doc+, doc-) = \"\n",
    "    f\"({triplets_dataset[0][0]}, {triplets_dataset[0][1]}, {triplets_dataset[0][2]})\"\n",
    ")\n",
    "\n",
    "# DocumentDataset sample print\n",
    "print(\"\\ndoc_emb:\", dd_dataset[0][0], \"\\ndocid_emb:\", dd_dataset[0][1])\n",
    "print(\"decoded docid:\", dd_dataset.decode_docid(dd_dataset[0][1]))\n",
    "print(\"decoded doc:\", dd_dataset.tokenizer.decode(dd_dataset[0][0]))\n",
    "\n",
    "# RetrievalDataset sample print\n",
    "print(\"\\nquery_emb:\", ret_dataset[0][0], \"\\ndocid_emb:\", ret_dataset[0][1])\n",
    "print(\"decoded docid:\", ret_dataset.decode_docid(ret_dataset[0][1]))\n",
    "print(\"decoded query:\", ret_dataset.tokenizer.decode(ret_dataset[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: /Users/alirezarafiei/IR_DSI_project/lightning_logs\n",
      "\n",
      "  | Name            | Type       | Params\n",
      "-----------------------------------------------\n",
      "0 | siamese_network | Sequential | 2.0 K \n",
      "1 | fc_layers       | Sequential | 984 K \n",
      "2 | fc              | Linear     | 9     \n",
      "3 | sigmoid         | Sigmoid    | 0     \n",
      "4 | criterion       | BCELoss    | 0     \n",
      "-----------------------------------------------\n",
      "986 K     Trainable params\n",
      "0         Non-trainable params\n",
      "986 K     Total params\n",
      "3.945     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d7b877e920743e8b17134cda065a393",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alirezarafiei/miniforge3/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:436: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss_avg:  tensor(0.7042, device='mps:0')\n",
      "accuracy:  0.48486328125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alirezarafiei/miniforge3/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:436: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62dec2473047407b8b7409b7b36c2bab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f41ac2436f84d36a0c18397e34e3731",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss_avg:  tensor(0.6941, device='mps:0')\n",
      "val_loss_avg:  tensor(0.6932, device='mps:0')\n",
      "accuracy:  0.4942657734666552\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21619d269fc740168154ca7bf5403bf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss_avg:  tensor(0.6935, device='mps:0')\n",
      "val_loss_avg:  tensor(0.6932, device='mps:0')\n",
      "accuracy:  0.49805508128234316\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8b7fd6ab8fe48e4af1bf38badb7fcd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss_avg:  tensor(0.6933, device='mps:0')\n",
      "val_loss_avg:  tensor(0.6932, device='mps:0')\n",
      "accuracy:  0.5014484311853137\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ERROR: Failed to launch TensorBoard (exited with 1).\n",
       "Contents of stderr:\n",
       "I0427 16:55:25.226720 12280950784 plugin.py:429] Monitor runs begin\n",
       "Address already in use\n",
       "Port 6006 is in use by another program. Either identify and stop that program, or start the server with a different port."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Change dataset return type to 'emb' (w2v embeddings)\n",
    "pairs_dataset.set_ret_type('emb')\n",
    "\n",
    "# Initialize the siamese network\n",
    "siamese_net = siamese.SiameseNetwork(\n",
    "    input_size=EMBEDDING_SIZE,\n",
    "    conv_channels=[8,16,32])\n",
    "\n",
    "# Train the model\n",
    "train_set_sn, valid_set_sn, metrics = utils.train_model(\n",
    "    pairs_dataset, siamese_net, max_epochs=3,\n",
    "    batch_size=1024, split_ratio=0.8, num_workers=2)\n",
    "\n",
    "%tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6265e90c877d4ff88b1c755786b80211",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing Top 10 DocIDs:   0%|          | 0/67869 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ids: ['1142358' '2742763' '6114991' '8745760' '6841142' '8034100' '7670442'\n",
      " '8261837' '7093300' '8261838']\n",
      "Relevant ids: ['7867446', '3368049', '4778576', '2914344', '2045120', '7867439', '6110464', '2045118', '7867440', '8699805']\n",
      "Common elements 0\n"
     ]
    }
   ],
   "source": [
    "# Set model to evaluation mode\n",
    "siamese_net.eval()\n",
    "\n",
    "# Extract first query\n",
    "query = list(queries.keys())[0]\n",
    "# Get the list of relevant docids for the query\n",
    "docids_list = queries[query]['docids_list']\n",
    "\n",
    "# Compute top-k docids\n",
    "top_k_ids = eval_utils.top_k_docids_siamese(\n",
    "    siamese_net, queries[query], documents, k=K)\n",
    "\n",
    "# Print top-k docids and relevant docid\n",
    "print(f\"Top {K} ids:\", top_k_ids)\n",
    "print(\"Relevant ids:\", docids_list)\n",
    "print(\"Common elements\", np.isin(top_k_ids, docids_list).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f06cf4885934184bff35b35741b414d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing Mean Metrics:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "838c1df15d1649f6a3e2ab29683dff0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing Top 1000 DocIDs:   0%|          | 0/67869 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4db0f2b36b34f51a43589dadcb893b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing Top 1000 DocIDs:   0%|          | 0/67869 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b09a4085dd44e28afe947635a97934c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing Top 1000 DocIDs:   0%|          | 0/67869 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35c93c3607964d9fb0aa65e295e923dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing Top 1000 DocIDs:   0%|          | 0/67869 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "679c83d1970443bf8455f144ee29213c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing Top 1000 DocIDs:   0%|          | 0/67869 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72a9bb75eba1467eacde98a9d3e86f5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing Top 1000 DocIDs:   0%|          | 0/67869 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "951c46c614664f0db3740075a885e530",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing Top 1000 DocIDs:   0%|          | 0/67869 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90c8a8b77e694bf287496297fd9ca06b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing Top 1000 DocIDs:   0%|          | 0/67869 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64f7d5a1c30e46c6adf6bc96dcd5e672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing Top 1000 DocIDs:   0%|          | 0/67869 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "008b7462d76a41ec9470a537e58f7c4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing Top 1000 DocIDs:   0%|          | 0/67869 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Average Precision (MAP): 0.0\n",
      "Mean Precison@10 0.0\n",
      "Mean Recall@1000 0.0\n"
     ]
    }
   ],
   "source": [
    "# Change return type to 'id' to get query IDs\n",
    "pairs_dataset.set_ret_type('id')\n",
    "# Randomly select N triplets from validation set\n",
    "selected_triplets = random.sample(list(valid_set_sn), 10)\n",
    "# Extract queries from the selected triplets\n",
    "random_queries = [triplet[0] for triplet in selected_triplets]\n",
    "\n",
    "# Compute mean metrics\n",
    "running_mean_AP, running_mean_PatK, running_mean_RatK = eval_utils.compute_Mean_metrics(\n",
    "    siamese_net, random_queries, queries, documents, k=K, model_type='siamese')\n",
    "# Print mean average precision\n",
    "print(\"Mean Average Precision (MAP):\", running_mean_AP)\n",
    "# Print mean precision at k\n",
    "print(f\"Mean Precison@{K}\", running_mean_PatK)\n",
    "# Print mean recall at k\n",
    "print(f\"Mean Recall@1000\", running_mean_RatK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change return type to 'id' to get query IDs\n",
    "pairs_dataset.set_ret_type('id')\n",
    "# Randomly select N triplets from train set\n",
    "selected_triplets = random.sample(list(train_set_sn), 10)\n",
    "# Extract queries from the selected triplets\n",
    "random_queries = [triplet[0] for triplet in selected_triplets]\n",
    "\n",
    "# Compute mean metrics\n",
    "running_mean_AP, running_mean_PatK, running_mean_RatK = eval_utils.compute_Mean_metrics(\n",
    "    siamese_net, random_queries, queries, documents, k=K, model_type='siamese')\n",
    "# Print mean average precision\n",
    "print(\"Mean Average Precision (MAP):\", running_mean_AP)\n",
    "# Print mean precision at k\n",
    "print(f\"Mean Precison@{K}\", running_mean_PatK)\n",
    "# Print mean recall at k\n",
    "print(f\"Mean Recall@1000\", running_mean_RatK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name        | Type              | Params\n",
      "--------------------------------------------------\n",
      "0 | transformer | CustomTransformer | 3.4 M \n",
      "--------------------------------------------------\n",
      "3.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.4 M     Total params\n",
      "13.443    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2805dd06297d4f17992dc1eefd5f5610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alirezarafiei/miniforge3/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=13` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss_avg:  tensor(0.7741, device='mps:0')\n",
      "accuracy:  0.51416015625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alirezarafiei/miniforge3/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=13` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e3211341f034093a59953ecadcdaa23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbcaff94934e459885522da9e5049397",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss_avg:  tensor(0.6338, device='mps:0')\n",
      "val_loss_avg:  tensor(0.6173, device='mps:0')\n",
      "accuracy:  0.6883103826216289\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87319084908e4eb58ced44fc5d90ab40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss_avg:  tensor(0.6083, device='mps:0')\n",
      "val_loss_avg:  tensor(0.6147, device='mps:0')\n",
      "accuracy:  0.692743895309312\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "108bfff87dc24d798fd9dca31ae182a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss_avg:  tensor(0.6003, device='mps:0')\n",
      "val_loss_avg:  tensor(0.6166, device='mps:0')\n",
      "accuracy:  0.6908153891563416\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f42d9db81c84fa19bad837918b0a824",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alirezarafiei/miniforge3/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "# Change dataset return type to 'emb' (embeddings)\n",
    "pairs_dataset.set_ret_type('first_L_emb')\n",
    "\n",
    "# Initialize siamese transformer network\n",
    "siamese_transformer = attention_siamese.SiameseTransformer(\n",
    "    embedding_size=EMBEDDING_SIZE*MAX_TOKENS)\n",
    "\n",
    "# Train the model\n",
    "train_set_st, valid_set_st = utils.train_model(\n",
    "    pairs_dataset, siamese_transformer,\n",
    "    max_epochs=10, batch_size=1024, split_ratio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch-tb-profiler\n",
      "  Obtaining dependency information for torch-tb-profiler from https://files.pythonhosted.org/packages/46/d0/891ec43349f287ea5c313ebc1320e4eda38ccd4c7a0951657213467eaab5/torch_tb_profiler-0.4.3-py3-none-any.whl.metadata\n",
      "  Downloading torch_tb_profiler-0.4.3-py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: pandas>=1.0.0 in /Users/alirezarafiei/miniforge3/lib/python3.10/site-packages (from torch-tb-profiler) (2.2.0)\n",
      "Requirement already satisfied: tensorboard!=2.1.0,>=1.15 in /Users/alirezarafiei/miniforge3/lib/python3.10/site-packages (from torch-tb-profiler) (2.13.0)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /Users/alirezarafiei/miniforge3/lib/python3.10/site-packages (from pandas>=1.0.0->torch-tb-profiler) (1.24.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/alirezarafiei/miniforge3/lib/python3.10/site-packages (from pandas>=1.0.0->torch-tb-profiler) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/alirezarafiei/miniforge3/lib/python3.10/site-packages (from pandas>=1.0.0->torch-tb-profiler) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/alirezarafiei/miniforge3/lib/python3.10/site-packages (from pandas>=1.0.0->torch-tb-profiler) (2023.4)\n",
      "Requirement already satisfied: absl-py>=0.4 in /Users/alirezarafiei/miniforge3/lib/python3.10/site-packages (from tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /Users/alirezarafiei/miniforge3/lib/python3.10/site-packages (from tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (1.60.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/alirezarafiei/miniforge3/lib/python3.10/site-packages (from tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (2.26.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /Users/alirezarafiei/miniforge3/lib/python3.10/site-packages (from tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/alirezarafiei/miniforge3/lib/python3.10/site-packages (from tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (3.5.2)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in /Users/alirezarafiei/miniforge3/lib/python3.10/site-packages (from tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (4.23.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/alirezarafiei/miniforge3/lib/python3.10/site-packages (from tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (2.31.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/alirezarafiei/miniforge3/lib/python3.10/site-packages (from tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (69.0.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/alirezarafiei/miniforge3/lib/python3.10/site-packages (from tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/alirezarafiei/miniforge3/lib/python3.10/site-packages (from tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (3.0.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /Users/alirezarafiei/miniforge3/lib/python3.10/site-packages (from tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (0.42.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/alirezarafiei/miniforge3/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/alirezarafiei/miniforge3/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/alirezarafiei/miniforge3/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/alirezarafiei/miniforge3/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (1.3.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/alirezarafiei/miniforge3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->torch-tb-profiler) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/alirezarafiei/miniforge3/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/alirezarafiei/miniforge3/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/alirezarafiei/miniforge3/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/alirezarafiei/miniforge3/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (2023.11.17)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/alirezarafiei/miniforge3/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (2.1.4)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /Users/alirezarafiei/miniforge3/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/alirezarafiei/miniforge3/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard!=2.1.0,>=1.15->torch-tb-profiler) (3.2.2)\n",
      "Downloading torch_tb_profiler-0.4.3-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torch-tb-profiler\n",
      "Successfully installed torch-tb-profiler-0.4.3\n"
     ]
    }
   ],
   "source": [
    "# Set model to evaluation mode\n",
    "siamese_transformer.eval()\n",
    "\n",
    "# Extract first query\n",
    "query = list(queries.keys())[0]\n",
    "# Get the list of relevant docids for the query\n",
    "docids_list = queries[query]['docids_list']\n",
    "\n",
    "# Compute top-k docids\n",
    "top_k_ids = eval_utils.top_k_docids_siamese(\n",
    "    siamese_transformer, queries[query], documents, k=K)\n",
    "\n",
    "# Print top-k docids and relevant docid\n",
    "print(f\"Top {K} ids:\", top_k_ids)\n",
    "print(\"Relevant ids:\", docids_list)\n",
    "print(\"Common elements\", np.isin(top_k_ids, docids_list).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change return type to 'id' to get query IDs\n",
    "pairs_dataset.set_ret_type('id')\n",
    "# Randomly select N triplets from validation set\n",
    "selected_triplets = random.sample(list(valid_set_st), 10)\n",
    "# Extract queries from the selected triplets\n",
    "random_queries = [triplet[0] for triplet in selected_triplets]\n",
    "\n",
    "# Compute mean metrics\n",
    "running_mean_AP, running_mean_PatK, running_mean_RatK = eval_utils.compute_Mean_metrics(\n",
    "    siamese_transformer, random_queries, queries,\n",
    "    documents, k=K, model_type='siamese')\n",
    "# Print mean average precision\n",
    "print(\"Mean Average Precision (MAP):\", running_mean_AP)\n",
    "# Print mean precision at k\n",
    "print(f\"Mean Precison@{K}\", running_mean_PatK)\n",
    "# Print mean recall at k\n",
    "print(f\"Mean Recall@1000\", running_mean_RatK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change return type to 'id' to get query IDs\n",
    "pairs_dataset.set_ret_type('id')\n",
    "# Randomly select N triplets from validation set\n",
    "selected_triplets = random.sample(list(train_set_st), 10)\n",
    "# Extract queries from the selected triplets\n",
    "random_queries = [triplet[0] for triplet in selected_triplets]\n",
    "\n",
    "# Compute mean metrics\n",
    "running_mean_AP, running_mean_PatK, running_mean_RatK = eval_utils.compute_Mean_metrics(\n",
    "    siamese_transformer, random_queries, queries,\n",
    "    documents, k=K, model_type='siamese')\n",
    "# Print mean average precision\n",
    "print(\"Mean Average Precision (MAP):\", running_mean_AP)\n",
    "# Print mean precision at k\n",
    "print(f\"Mean Precison@{K}\", running_mean_PatK)\n",
    "# Print mean recall at k\n",
    "print(f\"Mean Recall@1000\", running_mean_RatK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change dataset return type to 'emb' (embeddings)\n",
    "triplets_dataset.set_ret_type('emb')\n",
    "\n",
    "# Initialize the Siamese Lightning module\n",
    "siamese_lightning_module = siamese_triplet.SiameseNetworkPL(\n",
    "    input_size=EMBEDDING_SIZE, output_size=64,\n",
    "    margin=1.0, learning_rate=0.001, arch_type='linear')\n",
    "\n",
    "# Train the model\n",
    "train_set_sc, valid_set_sc = utils.train_model(\n",
    "    triplets_dataset, siamese_lightning_module,\n",
    "    max_epochs=10, batch_size=1024, split_ratio=0.8)\n",
    "\n",
    "# Start TensorBoard within the notebook\n",
    "%tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pyterrier-based approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTerrier 0.10.0 has loaded Terrier 5.8 (built by craigm on 2023-11-01 18:05) and terrier-helper 0.0.8\n",
      "\n",
      "No etc/terrier.properties, using terrier.default.properties for bootstrap configuration.\n"
     ]
    }
   ],
   "source": [
    "import pyterrier as pt\n",
    "if not pt.started():\n",
    "    pt.init()\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = pd.DataFrame([\n",
    "    {\"docno\": doc_id, \"text\": details['raw']}\n",
    "    for doc_id, details in documents.items()\n",
    "])\n",
    "\n",
    "indexer = pt.DFIndexer(\"./materials/index_pt/index\", overwrite=True)\n",
    "index_ref = indexer.index(docs[\"text\"], docs[\"docno\"])\n",
    "\n",
    "quer = pd.DataFrame([{\"qid\": str(qid), \"query\": details['raw']} for qid, details in queries.items()])\n",
    "qrels = pd.DataFrame([{\"qid\": str(qid), \"docno\": docno, \"label\": 1} for qid, details in queries.items() for docno in details['docids_list']])\n",
    "\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "def remove_punctuation(text):\n",
    "    text_no_punctuation = text.translate(translator)\n",
    "    return text_no_punctuation\n",
    "\n",
    "quer['query'] = quer['query'].apply(remove_punctuation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt.Experiment:   0%|          | 0/3 [00:00<?, ?system/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt.Experiment: 100%|██████████| 3/3 [03:05<00:00, 61.91s/system]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         name     map    ndcg     P@5    P@10    P@15    P@20    P@30  P@100  \\\n",
      "0    BR(BM25)  0.9456  0.9751  0.9614  0.9167  0.6399  0.4853  0.3264  0.099   \n",
      "1  BR(TF_IDF)  0.9458  0.9752  0.9617  0.9171  0.6400  0.4854  0.3264  0.099   \n",
      "2     BR(PL2)  0.9378  0.9720  0.9552  0.9043  0.6368  0.4843  0.3261  0.099   \n",
      "\n",
      "    P@200  ...  P@1000     R@5    R@10    R@15    R@20    R@30   R@100  \\\n",
      "0  0.0496  ...    0.01  0.4807  0.9167  0.9599  0.9706  0.9791  0.9896   \n",
      "1  0.0496  ...    0.01  0.4808  0.9171  0.9601  0.9708  0.9792  0.9896   \n",
      "2  0.0496  ...    0.01  0.4776  0.9043  0.9553  0.9686  0.9784  0.9896   \n",
      "\n",
      "    R@200   R@500  R@1000  \n",
      "0  0.9923  0.9944  0.9952  \n",
      "1  0.9922  0.9944  0.9952  \n",
      "2  0.9924  0.9944  0.9952  \n",
      "\n",
      "[3 rows x 21 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the index\n",
    "index = pt.IndexFactory.of(index_ref)\n",
    "\n",
    "# Create a BM25 retrieval model\n",
    "bm25 = pt.BatchRetrieve(index, wmodel=\"BM25\")\n",
    "\n",
    "tf_idf = pt.BatchRetrieve(index, wmodel=\"TF_IDF\")\n",
    "\n",
    "pl2 = pt.BatchRetrieve(index, wmodel=\"PL2\")\n",
    "\n",
    "evaluation = pt.Experiment(\n",
    "    [bm25, tf_idf, pl2],\n",
    "    quer,\n",
    "    qrels,\n",
    "    eval_metrics=['map', 'ndcg', 'P', 'recall'],\n",
    "    round=4,\n",
    "    verbose=\"true\"\n",
    ")\n",
    "\n",
    "print(evaluation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
