{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: jupyter [-h] [--version] [--config-dir] [--data-dir] [--runtime-dir]\n",
      "               [--paths] [--json] [--debug]\n",
      "               [subcommand]\n",
      "\n",
      "Jupyter: Interactive Computing\n",
      "\n",
      "positional arguments:\n",
      "  subcommand     the subcommand to launch\n",
      "\n",
      "options:\n",
      "  -h, --help     show this help message and exit\n",
      "  --version      show the versions of core jupyter packages and exit\n",
      "  --config-dir   show Jupyter config dir\n",
      "  --data-dir     show Jupyter data dir\n",
      "  --runtime-dir  show Jupyter runtime dir\n",
      "  --paths        show all Jupyter paths. Add --json for machine-readable\n",
      "                 format.\n",
      "  --json         output paths as machine-readable json\n",
      "  --debug        output debug information about paths\n",
      "\n",
      "Available subcommands: console dejavu events execute kernel kernelspec lab\n",
      "labextension labhub migrate nbconvert notebook qtconsole run server\n",
      "troubleshoot trust\n",
      "\n",
      "Jupyter command `jupyter-nbextension` not found.\n"
     ]
    }
   ],
   "source": [
    "#!pip install nltk\n",
    "#!pip install pytorch_lightning\n",
    "#!pip install gensim\n",
    "#!pip install pyserini==0.12.0\n",
    "#!pip install python-terrier\n",
    "#!pip install ipywidgets\n",
    "#!pip install --upgrade notebook jupyter\n",
    "#!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from src.dataset import mydataset\n",
    "from src.utils import utils, dataset_utils, eval_utils, trie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/alirezarafiei/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/alirezarafiei/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/alirezarafiei/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 120\n",
    "MAX_TOKENS = 7\n",
    "K = 10      # K param metrics\n",
    "\n",
    "QUERIES_PATH = './materials/queries.json'\n",
    "DOCS_PATH = './materials/documents.json'\n",
    "W2V_PATH = './materials/word2vec_model.bin'\n",
    "BT_MOD_PATH = './materials/bt_model.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to initialize pre-built index msmarco-passage.\n",
      "/Users/alirezarafiei/.cache/pyserini/indexes/index-msmarco-passage-20201117-f87c94.1efad4f1ae6a77e235042eff4be1612d already exists, skipping download.\n",
      "Initializing msmarco-passage...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building dictionaries: 100%|██████████| 6980/6980 [01:26<00:00, 80.77it/s]\n"
     ]
    }
   ],
   "source": [
    "# If the json files of documents does not exists\n",
    "if not os.path.exists(QUERIES_PATH) or not os.path.exists(DOCS_PATH) or not os.path.exists(W2V_PATH):\n",
    "    # Build dictionaries and corpus\n",
    "    queries, documents, corpus = dataset_utils.build_dicts(\n",
    "        max_topics=None, max_docs=10)\n",
    "    # Write to file the dictionaries\n",
    "    with open(QUERIES_PATH, 'w') as json_file:\n",
    "        json.dump(queries, json_file)\n",
    "    with open(DOCS_PATH, 'w') as json_file:\n",
    "        json.dump(documents, json_file)\n",
    "\n",
    "    # Train the word2vec model\n",
    "    w2v_model = Word2Vec(\n",
    "        sentences=corpus, vector_size=EMBEDDING_SIZE,\n",
    "        window=MAX_TOKENS if MAX_TOKENS else 5,\n",
    "        min_count=1, sg=0, epochs=10)\n",
    "    \n",
    "    # Save w2v model\n",
    "    w2v_model.save(W2V_PATH)\n",
    "\n",
    "# Read from json\n",
    "with open(QUERIES_PATH, 'r') as json_file:\n",
    "    queries = json.load(json_file)\n",
    "with open(DOCS_PATH, 'r') as json_file:\n",
    "    documents = json.load(json_file)\n",
    "\n",
    "# Load model\n",
    "w2v_model = Word2Vec.load(W2V_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding queries: 100%|██████████| 6980/6980 [00:00<00:00, 12184.35it/s]\n"
     ]
    }
   ],
   "source": [
    "# Compute w2v embeddings for queries\n",
    "for id in tqdm(queries, \"Embedding queries\"):\n",
    "    raw_query = queries[id]['raw']\n",
    "    queries[id]['emb'] = dataset_utils.compute_embedding(\n",
    "        raw_query, w2v_model, MAX_TOKENS)\n",
    "    queries[id]['first_L_emb'] = dataset_utils.compute_embedding(\n",
    "        raw_query, w2v_model, MAX_TOKENS)\n",
    "\n",
    "# Compute w2v embeddings for documents\n",
    "for docid in tqdm(documents, \"Embedding documents\"):\n",
    "    raw_doc = documents[docid]['raw']\n",
    "    documents[docid]['emb'] = dataset_utils.compute_embedding(\n",
    "        raw_doc, w2v_model)\n",
    "    documents[docid]['first_L_emb'] = dataset_utils.compute_embedding(\n",
    "        raw_doc, w2v_model, MAX_TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b299857ea368474f80ab74f16b82075a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building QueryDocumentDataset:   0%|          | 0/6980 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72ca4b4efbf44ef29233c8c90b20cbd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building TripletQueryDocumentDataset:   0%|          | 0/6980 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create datasets for siamese models\n",
    "pairs_dataset = mydataset.QueryDocumentDataset(queries, documents)\n",
    "triplets_dataset = mydataset.TripletQueryDocumentDataset(queries, documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f509c688b5e04d00a04599e54ceb96a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cdcbc9beb464b49a121a4ad558630d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eadb22e58f37415cb3c632cd293525cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d533fb972ea425db5bcf4fc9a30b940",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building DocumentDataset:   0%|          | 0/67869 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecda124d3b6e4f0e867857686c0f871a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building RetrievalDataset:   0%|          | 0/6980 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create datasets for seq2seq models\n",
    "dd_dataset = mydataset.DocumentDataset(documents)\n",
    "ret_dataset = mydataset.RetrievalDataset(documents, queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(query, doc, relevance) = (1102330, 2914344, 1)\n",
      "(query, doc+, doc-) = (1102330, 2914344, 2340319)\n",
      "\n",
      "doc_emb: tensor([ 5696,  8325,   819,  1836,  9657,  1406,  7178, 19511,   819,  1836,\n",
      "           78,    60,  9657, 20743,  3841,   706,   228,     3,  9052,   226,\n",
      "          159,    51,  3841, 20743,   158,    32,   102,    40, 20743,  3841,\n",
      "         3841,     1]) \n",
      "docid_emb: tensor([12,  7,  8,  6,  7,  4,  4,  6, 10, 11, 11])\n",
      "decoded docid: 7867446\n",
      "decoded doc: suffer frequent headach jaw pain wake dull headach sore jaw grind teeth night could bruxism teeth grind peopl grind teeth teeth</s>\n",
      "\n",
      "query_emb: tensor([  158,    32,   102,    40, 20743,  3841,  2085,     1,     0]) \n",
      "docid_emb: tensor([12,  7,  8,  6,  7,  4,  4,  6, 10, 11, 11])\n",
      "decoded docid: 7867446\n",
      "decoded query: peopl grind teeth sleep</s><pad>\n"
     ]
    }
   ],
   "source": [
    "# QueryDocumentDataset sample print (by default, IDs will be printed)\n",
    "print(\n",
    "    f\"(query, doc, relevance) = \"\n",
    "    f\"({pairs_dataset[0][0]}, {pairs_dataset[0][1]}, {pairs_dataset[0][2]})\"\n",
    ")\n",
    "\n",
    "# TripletQueryDocumentDataset sample print (by default, IDs will be printed)\n",
    "print(\n",
    "    f\"(query, doc+, doc-) = \"\n",
    "    f\"({triplets_dataset[0][0]}, {triplets_dataset[0][1]}, {triplets_dataset[0][2]})\"\n",
    ")\n",
    "\n",
    "# DocumentDataset sample print\n",
    "print(\"\\ndoc_emb:\", dd_dataset[0][0], \"\\ndocid_emb:\", dd_dataset[0][1])\n",
    "print(\"decoded docid:\", dd_dataset.decode_docid(dd_dataset[0][1]))\n",
    "print(\"decoded doc:\", dd_dataset.tokenizer.decode(dd_dataset[0][0]))\n",
    "\n",
    "# RetrievalDataset sample print\n",
    "print(\"\\nquery_emb:\", ret_dataset[0][0], \"\\ndocid_emb:\", ret_dataset[0][1])\n",
    "print(\"decoded docid:\", ret_dataset.decode_docid(ret_dataset[0][1]))\n",
    "print(\"decoded query:\", ret_dataset.tokenizer.decode(ret_dataset[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pyterrier-based approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTerrier 0.10.0 has loaded Terrier 5.8 (built by craigm on 2023-11-01 18:05) and terrier-helper 0.0.8\n",
      "\n",
      "No etc/terrier.properties, using terrier.default.properties for bootstrap configuration.\n"
     ]
    }
   ],
   "source": [
    "import pyterrier as pt\n",
    "if not pt.started():\n",
    "    pt.init()\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = pd.DataFrame([\n",
    "    {\"docno\": doc_id, \"text\": details['raw']}\n",
    "    for doc_id, details in documents.items()\n",
    "])\n",
    "\n",
    "indexer = pt.DFIndexer(\"./materials/index_pt/index\", overwrite=True)\n",
    "index_ref = indexer.index(docs[\"text\"], docs[\"docno\"])\n",
    "\n",
    "quer = pd.DataFrame([{\"qid\": str(qid), \"query\": details['raw']} for qid, details in queries.items()])\n",
    "qrels = pd.DataFrame([{\"qid\": str(qid), \"docno\": docno, \"label\": 1} for qid, details in queries.items() for docno in details['docids_list']])\n",
    "\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "def remove_punctuation(text):\n",
    "    text_no_punctuation = text.translate(translator)\n",
    "    return text_no_punctuation\n",
    "\n",
    "quer['query'] = quer['query'].apply(remove_punctuation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt.Experiment:   0%|          | 0/3 [00:00<?, ?system/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt.Experiment: 100%|██████████| 3/3 [03:05<00:00, 61.91s/system]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         name     map    ndcg     P@5    P@10    P@15    P@20    P@30  P@100  \\\n",
      "0    BR(BM25)  0.9456  0.9751  0.9614  0.9167  0.6399  0.4853  0.3264  0.099   \n",
      "1  BR(TF_IDF)  0.9458  0.9752  0.9617  0.9171  0.6400  0.4854  0.3264  0.099   \n",
      "2     BR(PL2)  0.9378  0.9720  0.9552  0.9043  0.6368  0.4843  0.3261  0.099   \n",
      "\n",
      "    P@200  ...  P@1000     R@5    R@10    R@15    R@20    R@30   R@100  \\\n",
      "0  0.0496  ...    0.01  0.4807  0.9167  0.9599  0.9706  0.9791  0.9896   \n",
      "1  0.0496  ...    0.01  0.4808  0.9171  0.9601  0.9708  0.9792  0.9896   \n",
      "2  0.0496  ...    0.01  0.4776  0.9043  0.9553  0.9686  0.9784  0.9896   \n",
      "\n",
      "    R@200   R@500  R@1000  \n",
      "0  0.9923  0.9944  0.9952  \n",
      "1  0.9922  0.9944  0.9952  \n",
      "2  0.9924  0.9944  0.9952  \n",
      "\n",
      "[3 rows x 21 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the index\n",
    "index = pt.IndexFactory.of(index_ref)\n",
    "\n",
    "# Create a BM25 retrieval model\n",
    "bm25 = pt.BatchRetrieve(index, wmodel=\"BM25\")\n",
    "\n",
    "tf_idf = pt.BatchRetrieve(index, wmodel=\"TF_IDF\")\n",
    "\n",
    "pl2 = pt.BatchRetrieve(index, wmodel=\"PL2\")\n",
    "\n",
    "evaluation = pt.Experiment(\n",
    "    [bm25, tf_idf, pl2],\n",
    "    quer,\n",
    "    qrels,\n",
    "    eval_metrics=['map', 'ndcg', 'P', 'recall'],\n",
    "    round=4,\n",
    "    verbose=\"true\"\n",
    ")\n",
    "\n",
    "print(evaluation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
